{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:黃梓寧\n",
    "\n",
    "Student ID:109061648\n",
    "\n",
    "GitHub ID:duncan1315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2020-Lab1-Master Repo](https://github.com/fhcalderon87/DM2020-Lab1-Master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2020-Lab1-Master Repo](https://github.com/fhcalderon87/DM2020-Lab1-Master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fhcalderon87/DM2020-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (Oct. 22th 11:59 pm, Thursday)__. "
   ]
  },
  {
   "source": [
    "### Begin Assignment Here\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST necessary for when working with external scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, \\\n",
    "                                  shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# my functions\n",
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "# construct dataframe from a list\n",
    "X = pd.DataFrame.from_records(dmh.format_rows(twenty_train), columns= ['text'])"
   ]
  },
  {
   "source": [
    "### ** >>> Exercise 2 (take home):** \n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their [documentation](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more information. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "X.query('category == \"1\"') #method1\n",
    "\n",
    "X.head() #method2, default=5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### >>> **Exercise 5 (take home)** \n",
    "There is an old saying that goes, \"The devil is in the details.\" When we are working with extremely large data, it's difficult to check records one by one (as we have been doing so far). And also, we don't even know what kind of missing values we are facing. Thus, \"debugging\" skills get sharper as we spend more time solving bugs. Let's focus on a different method to check for missing values and the kinds of missing values you may encounter. It's not easy to check for missing values as you will find out in a minute.\n",
    "\n",
    "Please check the data and the process below, describe what you observe and why it happened.   \n",
    "$Hint$ :  why `.isnull()` didn't work?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A is np.nan equal to missing value  \n",
    "B E None is NonType  \n",
    "C,D,F is  simply string, that's why .isnull() didn't work\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### >>> Exercise 6 (take home):\n",
    "Notice any changes to the `X` dataframe? What are they? Report every change you noticed as compared to the previous state of `X`. Feel free to query and look more closely at the dataframe for these changes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "input:  \n",
    "X_sample[::10][0:10]  \n",
    "  \n",
    "  output:  \n",
    "\ttext\tcategory\tcategory_name  \n",
    "\t1032\tFrom: mathew <mathew@mantis.co.uk> Subject: Re...\t0\talt.atheism  \n",
    "\t870\tFrom: ccgwt@trentu.ca (Grant Totten) Subject: ...\t1\tcomp.graphics  \n",
    "\t12\tFrom: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...\t0\talt.atheism  \n",
    "\t1892\tFrom: oehler@picard.cs.wisc.edu (Eric Oehler) ...\t1\tcomp.graphics  \n",
    "\t272\tFrom: rayssd!esther@uunet.uu.net (Esther A. Pa...\t3\tsoc.religion.christian  \n",
    "\t1102\tFrom: andrew@calvin.dgbt.doc.ca (Andrew Patric...\t2\tsci.med  \n",
    "\t1971\tFrom: rob@rjck.UUCP (Robert J.C. Kyanko) Subje...\t1\tcomp.graphics  \n",
    "\t2050\tFrom: keith@cco.caltech.edu (Keith Allan Schne...\t0\talt.atheism  \n",
    "\t303\tFrom: bolson@carson.u.washington.edu (Edward B...\t1\tcomp.graphics  \n",
    "\t1078\tFrom: young@serum.kodak.com (Rich Young) Subje...\t2\tsci.med  \n",
    "\t  \n",
    "\t  Dataframe is ordered randomly.  \n",
    "\t  It will sample the same file no matter how many time you try because random state is same\n",
    "\t  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### >>> **Exercise 8 (take home):** \n",
    "We can also do a side-by-side comparison of the distribution between the two datasets, but maybe you can try that as an excerise. Below we show you an snapshot of the type of chart we are looking for. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### >>> ** homework part 2:**  \n",
    "#New dataset code begin here#\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import plotly \n",
    "import nltk\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers.data_mining_helpers as dmh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000 \n\n748 \n\n1000 \n\n                                                   text  score\n0     So there is no way for me to plug it in here i...      0\n1                           Good case, Excellent value.      1\n2                                Great for the jawbone.      1\n3     Tied to charger for conversations lasting more...      0\n4                                     The mic is great.      1\n...                                                 ...    ...\n2743  I think food should have flavor and texture an...      0\n2744                           Appetite instantly gone.      0\n2745  Overall I was not impressed and would not go b...      0\n2746  The whole experience was underwhelming, and I ...      0\n2747  Then, as if I hadn't wasted enough of my life ...      0\n\n[2748 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\n",
    "    '/media/labhdd/Duncan/datamining/class/DM2020-Lab1-Homework1/dataset/sentiment labelled sentences/amazon_cells_labelled.txt',  sep='\\t',header=None)\n",
    "df2 = pd.read_csv(\n",
    "    '/media/labhdd/Duncan/datamining/class/DM2020-Lab1-Homework1/dataset/sentiment labelled sentences/imdb_labelled.txt',  sep='\\t',header=None)\n",
    "df3 = pd.read_csv(\n",
    "    '/media/labhdd/Duncan/datamining/class/DM2020-Lab1-Homework1/dataset/sentiment labelled sentences/yelp_labelled.txt',  sep='\\t',header=None)\n",
    "\n",
    "df = pd.concat([df1,df2,df3],axis=0,ignore_index = True)\n",
    "df.columns = ['text', 'score']\n",
    "\n",
    "print(len(df1),'\\n')\n",
    "print(len(df2),'\\n')\n",
    "print(len(df3),'\\n')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                text  score\n0  So there is no way for me to plug it in here i...      0\n1                        Good case, Excellent value.      1\n2                             Great for the jawbone.      1\n3  Tied to charger for conversations lasting more...      0\n4                                  The mic is great.      1\n\n\n text     Then, as if I hadn't wasted enough of my life ...\nscore                                                    0\nName: 2747, dtype: object\n\n\n Then, as if I hadn't wasted enough of my life there, they poured salt in the wound by drawing out the time it took to bring the check.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[:5])\n",
    "print('\\n\\n',df.iloc[2747]) #check last one\n",
    "print('\\n\\n',df.loc[2747,'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10 \n\n3 \n\n4 \n\n17\n"
     ]
    }
   ],
   "source": [
    "print(sum(df1.duplicated()),'\\n')\n",
    "print(sum(df2.duplicated()),'\\n')\n",
    "print(sum(df3.duplicated()),'\\n')\n",
    "print(sum(df.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text  score\n",
       "285                                       Great phone!.      1\n",
       "407                                        Works great.      1\n",
       "524                                       Works great!.      1\n",
       "543                             Don't buy this product.      0\n",
       "744   If you like a loud buzzing to override all you...      0\n",
       "748                                       Does not fit.      0\n",
       "778                               This is a great deal.      1\n",
       "792                                        Great Phone.      1\n",
       "892                    Excellent product for the price.      1\n",
       "896                                        Great phone.      1\n",
       "1165                   Definitely worth checking out.        1\n",
       "1387                                 Not recommended.        0\n",
       "1590                                            10/10        1\n",
       "2562                                 I love this place.      1\n",
       "2564                             The food was terrible.      0\n",
       "2591                                   I won't be back.      0\n",
       "2594                  I would not recommend this place.      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>285</th>\n      <td>Great phone!.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>407</th>\n      <td>Works great.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>524</th>\n      <td>Works great!.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>543</th>\n      <td>Don't buy this product.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>744</th>\n      <td>If you like a loud buzzing to override all you...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>748</th>\n      <td>Does not fit.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>778</th>\n      <td>This is a great deal.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>792</th>\n      <td>Great Phone.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>892</th>\n      <td>Excellent product for the price.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>Great phone.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1165</th>\n      <td>Definitely worth checking out.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1387</th>\n      <td>Not recommended.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1590</th>\n      <td>10/10</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2562</th>\n      <td>I love this place.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2564</th>\n      <td>The food was terrible.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2591</th>\n      <td>I won't be back.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2594</th>\n      <td>I would not recommend this place.</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14 \n\n2745 \n\n"
     ]
    }
   ],
   "source": [
    "df=df.drop(index=[524,792,896]) #I only delete text which is almost the same\n",
    "\n",
    "'''\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "'''\n",
    "print(sum(df.duplicated()),'\\n')\n",
    "print(len(df),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                   text  score\n285                                       Great phone!.      1\n407                                        Works great.      1\n543                             Don't buy this product.      0\n744   If you like a loud buzzing to override all you...      0\n748                                       Does not fit.      0\n778                               This is a great deal.      1\n892                    Excellent product for the price.      1\n1165                   Definitely worth checking out.        1\n1387                                 Not recommended.        0\n1590                                            10/10        1\n2562                                 I love this place.      1\n2564                             The food was terrible.      0\n2591                                   I won't be back.      0\n2594                  I would not recommend this place.      0 \n\n\n0       False\n1       False\n2       False\n3       False\n4       False\n        ...  \n2743    False\n2744    False\n2745    False\n2746    False\n2747    False\nLength: 2745, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(df[df.duplicated()],'\\n\\n') #I want to check how is the dupicated text looks like.\n",
    "print(df.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n=1000)\n",
    "print(len(sample),'\\n')\n",
    "print(sample[0:4],'\\n\\n')\n",
    "print(sample.iloc[999],'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.score.value_counts())\n",
    "\n",
    "df.score.value_counts().plot(kind = 'bar',\n",
    "                                    title = 'Category distribution',\n",
    "                                    ylim = [0, 1500],        \n",
    "                                    rot = 0, fontsize = 11, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.score.value_counts())\n",
    "\n",
    "sample.score.value_counts().plot(kind = 'bar',\n",
    "                                    title = 'Category distribution',\n",
    "                                    ylim = [0, 600],        \n",
    "                                    rot = 0, fontsize = 11, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unigrams'] = df['text'].apply(lambda x: dmh.tokenize_text(x))\n",
    "df[0:4][\"unigrams\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df[0:1]['unigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "df_counts = count_vect.fit_transform(df.text)\n",
    "\n",
    "df_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_name = count_vect.get_feature_names()\n",
    "\n",
    "print (feature_name,'\\n\\n')\n",
    "print (df_counts,'\\n\\n')\n",
    "print( df_counts[0:5, 0:100].toarray(),'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.transform(['zillion zombiez']).toarray() #make sure it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = [\"term_\"+str(i) for i in count_vect.get_feature_names()[0:50]]\n",
    "plot_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y = [\"text_\"+ str(i) for i in list(X.index)[0:50]]\n",
    "plot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_z = df_counts[0:50, 0:50].toarray()\n",
    "plot_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "plt.subplots(figsize=(18, 14))\n",
    "ax = sns.heatmap(df_todraw,\n",
    "                 cmap=\"PuRd\",\n",
    "                 vmin=0, vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_counts,'\\n')\n",
    "print(df_counts.shape,'\\n')\n",
    "print(type(df_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = PCA(n_components = 2).fit_transform(df_counts.toarray())\n",
    "print(df_reduced.shape)\n",
    "print('\\n',df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (25,10))\n",
    "ax = fig.subplots()\n",
    "\n",
    "for c, category in zip(col, categories):\n",
    "    xs = df_reduced[X['category_name'] == category].T[0]\n",
    "    ys = df_reduced[X['category_name'] == category].T[1]\n",
    "   \n",
    "    ax.scatter(xs, ys, c = c, marker='o')\n",
    "\n",
    "ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax.set_xlabel('\\nX Label')\n",
    "ax.set_ylabel('\\nY Label')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}